{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8n_svB-o_AS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa4a4acc"
      },
      "source": [
        "# Task\n",
        "Train multiple machine learning models on the Iris dataset, evaluate their performance using accuracy, precision, recall, and F1-score, implement hyperparameter tuning with GridSearchCV and RandomizedSearchCV, and select the best-performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aa858e6"
      },
      "source": [
        "## Load and preprocess data\n",
        "\n",
        "### Subtask:\n",
        "Load the Iris dataset from `sklearn.datasets`. Split the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6d78d26"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the Iris dataset and split it into training and testing sets as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "895728d4"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d80749ff"
      },
      "source": [
        "## Train multiple models\n",
        "\n",
        "### Subtask:\n",
        "Train several different machine learning models (e.g., Logistic Regression, Support Vector Machine, Decision Tree, Random Forest) on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51ffb54e"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary model classes and train each model on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff474280",
        "outputId": "b75629e0-cc94-4189-9745-640d0e0dc078"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate each model\n",
        "log_reg = LogisticRegression(max_iter=200) # Increased max_iter to prevent convergence warning\n",
        "svc = SVC()\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "# Train each model\n",
        "log_reg.fit(X_train, y_train)\n",
        "svc.fit(X_train, y_train)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Models trained successfully.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f3d748b"
      },
      "source": [
        "## Implement hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Use techniques like GridSearchCV or RandomizedSearchCV to find the optimal hyperparameters for each model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9da2386e"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary modules for hyperparameter tuning and define the parameter grids for each model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec586128"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# Define parameter grids for each model\n",
        "log_reg_param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "svc_param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "dt_param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30376fe6"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate and fit GridSearchCV or RandomizedSearchCV for each model using the defined parameter grids and the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0811de9",
        "outputId": "1b6c6fba-071c-4162-c68f-c749ec403f4e"
      },
      "source": [
        "# Instantiate and fit GridSearchCV for Logistic Regression\n",
        "log_reg_grid_search = GridSearchCV(LogisticRegression(max_iter=200), log_reg_param_grid, cv=5)\n",
        "log_reg_grid_search.fit(X_train, y_train)\n",
        "best_log_reg = log_reg_grid_search.best_estimator_\n",
        "\n",
        "# Instantiate and fit GridSearchCV for SVC\n",
        "svc_grid_search = GridSearchCV(SVC(), svc_param_grid, cv=5)\n",
        "svc_grid_search.fit(X_train, y_train)\n",
        "best_svc = svc_grid_search.best_estimator_\n",
        "\n",
        "# Instantiate and fit GridSearchCV for Decision Tree\n",
        "dt_grid_search = GridSearchCV(DecisionTreeClassifier(), dt_param_grid, cv=5)\n",
        "dt_grid_search.fit(X_train, y_train)\n",
        "best_dt_classifier = dt_grid_search.best_estimator_\n",
        "\n",
        "# Instantiate and fit RandomizedSearchCV for Random Forest (demonstrating both methods)\n",
        "rf_random_search = RandomizedSearchCV(RandomForestClassifier(), rf_param_grid, cv=5, n_iter=10) # Use n_iter for RandomizedSearch\n",
        "rf_random_search.fit(X_train, y_train)\n",
        "best_rf_classifier = rf_random_search.best_estimator_\n",
        "\n",
        "print(\"Hyperparameter tuning completed and best estimators stored.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter tuning completed and best estimators stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58acdc7e"
      },
      "source": [
        "## Evaluate model performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of each tuned model on the testing data using metrics such as accuracy, precision, recall, and F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c28d2f0b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary evaluation metrics and initialize a dictionary to store the performance metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d10e2be0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "performance_metrics = {}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a657a2e9"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate each best model on the test set and store the metrics in the performance dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70417367"
      },
      "source": [
        "# Evaluate Logistic Regression\n",
        "log_reg_pred = best_log_reg.predict(X_test)\n",
        "performance_metrics['Logistic Regression'] = {\n",
        "    'Accuracy': accuracy_score(y_test, log_reg_pred),\n",
        "    'Precision': precision_score(y_test, log_reg_pred, average='weighted'),\n",
        "    'Recall': recall_score(y_test, log_reg_pred, average='weighted'),\n",
        "    'F1-score': f1_score(y_test, log_reg_pred, average='weighted')\n",
        "}\n",
        "\n",
        "# Evaluate SVC\n",
        "svc_pred = best_svc.predict(X_test)\n",
        "performance_metrics['SVC'] = {\n",
        "    'Accuracy': accuracy_score(y_test, svc_pred),\n",
        "    'Precision': precision_score(y_test, svc_pred, average='weighted'),\n",
        "    'Recall': recall_score(y_test, svc_pred, average='weighted'),\n",
        "    'F1-score': f1_score(y_test, svc_pred, average='weighted')\n",
        "}\n",
        "\n",
        "# Evaluate Decision Tree\n",
        "dt_pred = best_dt_classifier.predict(X_test)\n",
        "performance_metrics['Decision Tree'] = {\n",
        "    'Accuracy': accuracy_score(y_test, dt_pred),\n",
        "    'Precision': precision_score(y_test, dt_pred, average='weighted'),\n",
        "    'Recall': recall_score(y_test, dt_pred, average='weighted'),\n",
        "    'F1-score': f1_score(y_test, dt_pred, average='weighted')\n",
        "}\n",
        "\n",
        "# Evaluate Random Forest\n",
        "rf_pred = best_rf_classifier.predict(X_test)\n",
        "performance_metrics['Random Forest'] = {\n",
        "    'Accuracy': accuracy_score(y_test, rf_pred),\n",
        "    'Precision': precision_score(y_test, rf_pred, average='weighted'),\n",
        "    'Recall': recall_score(y_test, rf_pred, average='weighted'),\n",
        "    'F1-score': f1_score(y_test, rf_pred, average='weighted')\n",
        "}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4f6f5bf"
      },
      "source": [
        "**Reasoning**:\n",
        "Print the performance metrics for each model to display the evaluation results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbd487af",
        "outputId": "a1f788c4-5cea-4898-c32c-2d10e827bb5f"
      },
      "source": [
        "for model_name, metrics in performance_metrics.items():\n",
        "    print(f\"Performance Metrics for {model_name}:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")\n",
        "    print(\"-\" * 30)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics for Logistic Regression:\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1-score: 1.0000\n",
            "------------------------------\n",
            "Performance Metrics for SVC:\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1-score: 1.0000\n",
            "------------------------------\n",
            "Performance Metrics for Decision Tree:\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1-score: 1.0000\n",
            "------------------------------\n",
            "Performance Metrics for Random Forest:\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1-score: 1.0000\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17d6721c"
      },
      "source": [
        "## Analyze and select the best model\n",
        "\n",
        "### Subtask:\n",
        "Compare the performance metrics of all the models and select the one that performs best based on the chosen evaluation criteria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "922f9cc7"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the performance_metrics dictionary and print the performance metrics for each model. Based on the metrics, acknowledge that all models performed equally well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34c54ac9",
        "outputId": "1fc9aba0-489d-43dc-e868-128ae6049bdf"
      },
      "source": [
        "for model_name, metrics in performance_metrics.items():\n",
        "    print(f\"Performance Metrics for {model_name}:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"Based on the performance metrics, all models achieved perfect scores on the test set.\")\n",
        "print(\"Therefore, based purely on these metrics, all models perform equally well on this specific test set.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics for Logistic Regression:\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1-score: 1.0000\n",
            "------------------------------\n",
            "Performance Metrics for SVC:\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1-score: 1.0000\n",
            "------------------------------\n",
            "Performance Metrics for Decision Tree:\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1-score: 1.0000\n",
            "------------------------------\n",
            "Performance Metrics for Random Forest:\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1-score: 1.0000\n",
            "------------------------------\n",
            "Based on the performance metrics, all models achieved perfect scores on the test set.\n",
            "Therefore, based purely on these metrics, all models perform equally well on this specific test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c55baf3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   All four machine learning models (Logistic Regression, SVC, Decision Tree, and Random Forest), after hyperparameter tuning using GridSearchCV or RandomizedSearchCV, achieved perfect scores (1.0000) for Accuracy, Precision, Recall, and F1-score on the test set of the Iris dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXYCxqI8riyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}